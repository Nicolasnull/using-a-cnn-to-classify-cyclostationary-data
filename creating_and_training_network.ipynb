{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Training a CNN\n",
    "\n",
    "Authors: Matthew Gilley, John Morgan and Nicolas Null<br>\n",
    "Written in Fall 2021\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import datetime as dt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataloaders Into Notebook\n",
    "Dataloaders were created and separated in using the load_cropped_images notebook. To control the sizes of each dataset, you must run that notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "here = os.getcwd()\n",
    "\n",
    "with gzip.open(os.path.join(here, 'test_cropped_dataloader.pkl')) as file:\n",
    "    test_dataloader = pickle.load(file)\n",
    "\n",
    "\n",
    "with gzip.open(os.path.join(here, 'validate_cropped_dataLoader.pkl')) as file:\n",
    "    validate_dataloader = pickle.load(file)\n",
    "    \n",
    "with gzip.open(os.path.join(here, 'train_cropped_dataloader.pkl')) as file:\n",
    "    train_dataloader = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the Number of Noise and RFI Instances in each Dataset\n",
    "This step is done so that the output files can include the exact number of RFI and Noise in each dataset. This may help in understanding the data when we review the output files created by testing. This is accomplished by iterating through each dataloader and counting the number of noise labels and the number of rfi labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "noise: 113\n",
      "rfi: 117\n",
      "total 230\n",
      "\n",
      "Train: \n",
      "noise: 351\n",
      "rfi: 337\n",
      "total 688\n",
      "\n",
      "Validate: \n",
      "noise: 110\n",
      "rfi: 119\n",
      "total 229\n"
     ]
    }
   ],
   "source": [
    "# getting test dataset info\n",
    "test_noise = 0\n",
    "test_rfi = 0\n",
    "for (images, labels) in test_dataloader.dataset:\n",
    "    if labels == 0:\n",
    "        test_noise+=1\n",
    "    else:\n",
    "        test_rfi+=1\n",
    "\n",
    "print('Test:')\n",
    "print(f'noise: {test_noise}')\n",
    "print(f'rfi: {test_rfi}')\n",
    "print(f'total {len(test_dataloader.dataset)}')\n",
    "\n",
    "# getting train dataset info\n",
    "train_noise = 0\n",
    "train_rfi = 0\n",
    "for (images, labels) in train_dataloader.dataset:\n",
    "    if labels == 0:\n",
    "        train_noise+=1\n",
    "    else:\n",
    "        train_rfi+=1\n",
    "\n",
    "print('\\nTrain: ')\n",
    "print(f'noise: {train_noise}')\n",
    "print(f'rfi: {train_rfi}')\n",
    "print(f'total {len(train_dataloader.dataset)}')\n",
    "\n",
    "# getting validate dataset info\n",
    "validate_noise = 0\n",
    "validate_rfi = 0\n",
    "for (images, labels) in validate_dataloader.dataset:\n",
    "    if labels == 0:\n",
    "        validate_noise+=1\n",
    "    else:\n",
    "        validate_rfi+=1\n",
    "\n",
    "print('\\nValidate: ')\n",
    "print(f'noise: {validate_noise}')\n",
    "print(f'rfi: {validate_rfi}')\n",
    "print(f'total {len(validate_dataloader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a CNN\n",
    "\n",
    "#### Overview\n",
    "A Convolutional Neural Network (CNN) is a classifier that works with images. A CNN consists of multiple layers starting with the convolutional layers which identify high level features of the image and ending with a Deep Neural Network (DNN) which classify the image. A very insightful, interactive breakdown of the different components can be found <a href=\"https://poloclub.github.io/cnn-explainer/\">here.</a>\n",
    "\n",
    "<br>\n",
    "CNN Diagram<br>\n",
    "<a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\">Source</a>\n",
    "<br>\n",
    "<img src=\"notebook_images/CNN_diagram.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layers\n",
    "Each convolutional layer begins with convolution where a kernel is run over the input image. Kernels extract features from the images that will help differentiate between different types of images. These kernels are modified each epoch of the training to help extract more important features to the classification. Parameters of the kernels that can be modified during training include padding, kernel size, stride and weights (values inside of the kernel).\n",
    "<br><br>\n",
    "Animation showing kernal running over a 2-dimensional array<br>\n",
    "<a href=\"https://analyticsindiamag.com/convolutional-neural-network-image-classification-overview/\"> Source</a><br>\n",
    "<img src=\"notebook_images/conv_layer.gif\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in each convolutional layer is the activation function. In our case we are using the ReLU function. The ReLU function is currently the modern standard for activation functions. Is is very computationally cheap and proven to be just as good as sigmoid and tanh functions. It also does not allow a negative output.\n",
    "\n",
    "<br>\n",
    "Screenshot of common activation functions<br>\n",
    "<a href=\"https://www.quora.com/What-is-ReLU-and-Softmax\">Source</a>\n",
    "<br>\n",
    "<img src=\"notebook_images/activation_functions.png\" width=\"300px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the convolutional layer is the pooling function. The pooling function helps reduce the size of the activations and in turn the size of the overall network while still keeping the important values. The pooling layer is set up by the stride (the distance the kernel moves), the size of the kernel, and the type of pooling (typically either average or max). In the case of a 2x2 kernel with stride 2, 75% of the activation values are discarded. This helps reduce the complexity of the overall model as well as avoiding overfitting.\n",
    "\n",
    "<br>\n",
    "Example of a maxpool of size 2x2 and stride 2<br>\n",
    "<a href=\"https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/\">Source</a>\n",
    "<br>\n",
    "<img src=\"notebook_images/pooling_function.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Network\n",
    "Here we use a neural net with 2 fully connected hidden layers. The input will the the results from the final convolutional layer. The outputs will be the classification of the image.\n",
    "\n",
    "<br>\n",
    "Diagram of a fully connected DNN<br>\n",
    "<a href=\"https://www.researchgate.net/figure/Example-of-fully-connected-neural-network_fig2_331525817\">Source</a><br>\n",
    "<img src=\"notebook_images/DNN_diagram.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model\n",
    "\n",
    "<img src=\"notebook_images/Conv_diagram.png\" alt=\"Our Model\"><br>**This figure is generated by adapting the code from <a href=\"https://github.com/gwding/draw_convnet\">https://github.com/gwding/draw_convnet<a><br><br>\n",
    "\n",
    "Our model is very heavily based off of a network created while taking <a href=\"https://www.udacity.com/course/deep-learning-pytorch--ud188\">this Udacity Course</a>. It contains 3 convolutional layers and a DNN with 2 fully connected hidden layers. Knowing that this structure was able to correctly classify different images in the CIFAR dataset made us believe it would work very well in our case of astronomical data screenshots. We first define all of the different layers that will be used in the __init__ method. Then in the forward function we lay out exactly how the image will progress through each of the layers we designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() # crates all of the Layer types used in CNN \n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1,16,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,32,3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(32,64,3,padding=1)\n",
    "\n",
    "        # Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # DNN layers\n",
    "        self.fc1 = nn.Linear(64*32*32, 512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        # Dropout to avoid overfitting\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x): # forward is the method called to run the image through the network\n",
    "        x = self.pool(F.relu(self.conv1(x))) # convolutional layer 1\n",
    "        x = self.pool(F.relu(self.conv2(x))) # convolutional layer 2\n",
    "        x = self.pool(F.relu(self.conv3(x))) # convolutional layer 3\n",
    "        x = x.view(-1, 64*32*32) # flatten results from 2d to 1d\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x)) # DNN layer 1\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) # DNN layer 2\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=65536, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Criterion and Optimizer\n",
    "Criterion and optimizer are used to make the model better during training. The criterion uses cross entropy loss to calculate the errors in the model. The optimizer we chose to use was stochastic gradient descent. This uses loss calculated by the criterion to gradually minimize the errors in the model.\n",
    "\n",
    "<br>\n",
    "Gradient Descent Animation<br>\n",
    "<a href=\"https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/\">Source</a><br>\n",
    "<img src=\"notebook_images/gradient_descent.gif\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Control\n",
    "This allows us to control whether or not we run the training on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THIS TO TRUE IF YOU WANT TO RUN IN CPU NO MATTER WHAT\n",
    "force_cpu = False\n",
    "\n",
    "use_cuda = False #don't touch this\n",
    "if torch.cuda.is_available() and not force_cpu:\n",
    "    use_cuda = True\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output File Formatting\n",
    "In this block we name an output file based on the time and date that the following cell is run. We also record the name of the file, size of the datasets and corresponding distributions of the classes, the model architecture, and the device (either CUDA or CPU) that was used to run this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get file name \n",
    "date_n_time = dt.datetime.now()\n",
    "time_stamp_created= date_n_time.strftime('%d%b%y-%H_%M_%S')\n",
    "file_name = 'output_files/'+ time_stamp_created + '-crop_test_results.txt'\n",
    "\n",
    "\n",
    "\n",
    "original_stdout = sys.stdout\n",
    "# This creates file and starts saving info to it\n",
    "with open(file_name, 'w') as file:\n",
    "    sys.stdout = file\n",
    "    print(file_name.replace('.txt', '')+':')\n",
    "    print(\"\\nSize of datasets:\")\n",
    "    print(f'Train Size: Total={len(train_dataloader.dataset)}  Noise={train_noise}  RFI={train_rfi}')\n",
    "    print(f'Test Size: Total={len(test_dataloader.dataset)}  Noise={test_noise}  RFI={test_rfi}')\n",
    "    print(f'Validate Size: Total={len(validate_dataloader.dataset)}  Noise={validate_noise}  RFI={validate_rfi}')\n",
    "    print('\\nMODEL:')\n",
    "    print(model)\n",
    "    print(f'\\nDevice Used: {device}')\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "We force all needed resources (the model and each image/label) to the device determined earlier. We then run through each epoch of training. Each image is run through the network forwards to calculate the loss using the criterion. Then, the model is modified using the optimizer. Each epoch's loss is calculated with the validation set so we can track the improvement of each epoch. The total training time and validation loss of each epoch are recorded into the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.686645 \tValidation Loss: 0.682298\n",
      "Epoch: 2 \tTraining Loss: 0.678822 \tValidation Loss: 0.668599\n",
      "Epoch: 3 \tTraining Loss: 0.664582 \tValidation Loss: 0.657706\n",
      "Epoch: 4 \tTraining Loss: 0.639913 \tValidation Loss: 0.617296\n",
      "Epoch: 5 \tTraining Loss: 0.612176 \tValidation Loss: 0.568960\n",
      "Epoch: 6 \tTraining Loss: 0.535464 \tValidation Loss: 0.521981\n",
      "Epoch: 7 \tTraining Loss: 0.458803 \tValidation Loss: 0.362220\n",
      "Epoch: 8 \tTraining Loss: 0.346498 \tValidation Loss: 0.267156\n",
      "Epoch: 9 \tTraining Loss: 0.235337 \tValidation Loss: 0.208578\n",
      "Epoch: 10 \tTraining Loss: 0.177920 \tValidation Loss: 0.130007\n",
      "Epoch: 11 \tTraining Loss: 0.128551 \tValidation Loss: 0.088900\n",
      "Epoch: 12 \tTraining Loss: 0.113239 \tValidation Loss: 0.093529\n",
      "Epoch: 13 \tTraining Loss: 0.095199 \tValidation Loss: 0.073047\n",
      "Epoch: 14 \tTraining Loss: 0.086488 \tValidation Loss: 0.061253\n",
      "Epoch: 15 \tTraining Loss: 0.075933 \tValidation Loss: 0.056305\n",
      "Epoch: 16 \tTraining Loss: 0.080351 \tValidation Loss: 0.060196\n",
      "Epoch: 17 \tTraining Loss: 0.068621 \tValidation Loss: 0.053880\n",
      "Epoch: 18 \tTraining Loss: 0.069776 \tValidation Loss: 0.055078\n",
      "Epoch: 19 \tTraining Loss: 0.068058 \tValidation Loss: 0.125176\n",
      "Epoch: 20 \tTraining Loss: 0.069462 \tValidation Loss: 0.049545\n"
     ]
    }
   ],
   "source": [
    "# push needed info to cuda\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "criterion.cpu()\n",
    "num_epochs = 20\n",
    "\n",
    "validation_loss_min = -float('inf')\n",
    "\n",
    "# Start timer\n",
    "start_timer = time.perf_counter()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    train = 1\n",
    "    num_train_batches = len(train_dataloader)\n",
    "    model.train()\n",
    "    for image, label in train_dataloader:\n",
    "        # Sends stuffs to cuda if available\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        #print(\"Training Batch:\", train, \"of\", num_train_batches)\n",
    "        train += 1\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*image.size(0)\n",
    "\n",
    "    num_validate_batches = len(validate_dataloader)\n",
    "    validate = 1\n",
    "    model.eval()\n",
    "    for image, label in validate_dataloader:\n",
    "        # Sends stuffs to cuda if availible\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        #print(\"Validation Batch:\", validate, \"of\", num_validate_batches)\n",
    "        validate += 1\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        validation_loss += loss.item()*image.size(0)\n",
    "\n",
    "    train_loss = train_loss/len(train_dataloader.dataset)\n",
    "    validation_loss = validation_loss/len(validate_dataloader.dataset)\n",
    "\n",
    "    # PRINT OUTPUTS TO FILE\n",
    "    original_stdout = sys.stdout\n",
    "    with open(file_name, 'a') as file:\n",
    "        sys.stdout = file\n",
    "        print('\\nRESULTS:')\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, validation_loss))\n",
    "        print('\\n')\n",
    "        sys.stdout = original_stdout\n",
    "        \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, validation_loss))\n",
    "\n",
    "stop_timer = time.perf_counter()\n",
    "original_stdout = sys.stdout\n",
    "with open(file_name, 'a') as file:\n",
    "    sys.stdout = file\n",
    "    print(f\"TIME TO TRAIN MODEL: {stop_timer-start_timer}\")\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model With Unseen Data\n",
    "Now that the model is trained, we can measure the performance of the model using the test dataloader. Since we are not modifying the model, we run this set with no gradations. This way we only test the model, not modify. We also can calculate the confusion matrix, accuracy score and f1 score to better visualize the performance of this model. All of these statistics are saved to the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[110   3]\n",
      " [  4 113]]\n",
      "Accuracy Score: 0.9695652173913043\n",
      "f1 Score: 0.9695652173913043\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1027317551167115\n"
     ]
    }
   ],
   "source": [
    "# force to CPU\n",
    "device = torch.device('cpu')\n",
    "model.cpu()\n",
    "\n",
    "num_test_batches = len(test_dataloader)\n",
    "test = 1\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "predicted_classes = np.empty(0,dtype=\"uint8\")\n",
    "actual_classes = np.empty(0,dtype=\"uint8\")\n",
    "with torch.no_grad():\n",
    "    for image, label in test_dataloader:\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        #print(\"Test Batch:\", test, \"of\", num_test_batches)\n",
    "        test += 1\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        test_loss += loss.item()*image.size(0)\n",
    "        \n",
    "        predicted_classes = np.append(predicted_classes, np.argmax(output, axis=1))\n",
    "        actual_classes = np.append(actual_classes, label)\n",
    "        \n",
    "accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "confusion_matrix = cm(actual_classes, predicted_classes)\n",
    "f1 = f1_score(actual_classes,predicted_classes, average='micro')\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix}\")\n",
    "print(f\"Accuracy Score: {accuracy}\")\n",
    "print(f'f1 Score: {f1}')\n",
    "\n",
    "## MAKES CONFUSION MATRIX GRAPHIC\n",
    "plt.title(\"Confusion Matrix\", fontsize=20)\n",
    "\n",
    "palette = sns.set_palette('pastel')\n",
    "\n",
    "sns.heatmap(cm(actual_classes, predicted_classes),cbar=False,annot=True, annot_kws={'size': 13}, fmt=\"d\", linewidth=.5, robust=True, xticklabels=['Noise','RFI'], yticklabels=['Noise', 'RFI'], cmap='Blues', vmax = 3225, vmin = 19)\n",
    "\n",
    "plt.xlabel('Predicted Class', fontsize=15)\n",
    "plt.ylabel('Actual Class', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(file_name.replace('.txt', '.jpg'))\n",
    "plt.show()\n",
    "\n",
    "## END OF CONFUSION MATRIX GRAPHIC\n",
    "\n",
    "test_loss = test_loss/len(test_dataloader.dataset)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "original_stdout = sys.stdout\n",
    "with open(file_name, 'a') as file:\n",
    "    sys.stdout = file\n",
    "    print()\n",
    "    print(\"RESULTS:\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix}\")\n",
    "    print(f\"Accuracy Score: {accuracy}\")\n",
    "    print(f'f1 Score: {f1}')\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    sys.stdout = original_stdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"output_files/\" + time_stamp_created + \"-crop_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c8ae7b7397383be484f509670a744b7a75aaa69a8c48f2fc2b876db86f77bea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
